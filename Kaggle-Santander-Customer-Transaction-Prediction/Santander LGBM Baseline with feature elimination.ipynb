{"cells":[{"metadata":{"_uuid":"fb9097ae88608c29c159c1443947bc699486827c"},"cell_type":"markdown","source":"<h1><center><font size=\"5\">Santander LGBM Baseline Prediction with feature elimination</font></center></h1>\n"},{"metadata":{"_uuid":"0f36c1c80b51c9e42180e18be0c8492f8733dec1"},"cell_type":"markdown","source":"# Load Packages and Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom numba import jit\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06cd64c0f3127126bdc26a2e6c58c776241a17d8","_kg_hide-output":true},"cell_type":"code","source":"train = pd.read_csv('../input/santander-fe-train-and-test/fe_train.csv')\ntest = pd.read_csv('../input/santander-fe-train-and-test/fe_test.csv')\ntarget = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"073d6a5c3be3de1954c5f50b42de85ffd9dce70e","_kg_hide-input":true},"cell_type":"code","source":"print (\"Test \",test.shape)\nprint (\"Train \",train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train['Unnamed: 0']\ndel test['Unnamed: 0']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [c for c in train.columns if c not in ['ID_code', 'target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc=RandomForestClassifier(n_estimators=50, max_depth=2, random_state=42)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n%time\nrfc.fit(train[features].values, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(rfc.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 230 least important features\nleast_imp_230_features_df = pd.DataFrame(np.transpose([features, rfc.feature_importances_]), columns=['Feature Name', 'Importance']).\\\nsort_values('Importance', ascending=False).tail(230)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns.values:\n    if col in least_imp_230_features_df['Feature Name'].values:\n        train = train.drop(labels=[col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reducing size of data\n\ndef reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            print(\"******************************\")\n            print(\"Column: \",col)\n            print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n            print(\"dtype after: \",props[col].dtype)\n            print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props, NAlist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, NAlist = reduce_mem_usage(train)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in test.columns.values:\n    if col in least_imp_230_features_df['Feature Name'].values:\n        test = test.drop(labels=[col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test, NAlist = reduce_mem_usage(test)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50977096e5862aae1e81ee95213a6a5795e33a01"},"cell_type":"markdown","source":"# Build the Light GBM Model"},{"metadata":{"trusted":true,"_uuid":"ce2a155cd34809f36d665ce38bb3ec632ca62746","_kg_hide-input":false},"cell_type":"code","source":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.335,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.041,\n    'learning_rate': 0.0083,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': -1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cf186957ca8ba2c018701178284190727edde87"},"cell_type":"code","source":"num_folds = 5\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target']]\nprint(features == test.columns.values)\n\nfolds = StratifiedKFold(n_splits=num_folds, shuffle=False, random_state=42)\noof = np.zeros(len(train))\n# getVal = np.zeros(len(train))\npredictions = np.zeros(len(target))\n# feature_importance_df = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    \n    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n    \n    print(\"Fold idx:{}\".format(fold_))\n    trn_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    \n    clf = lgb.train(param, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 4000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n#     fold_importance_df = pd.DataFrame()\n#     fold_importance_df[\"feature\"] = features\n#     fold_importance_df[\"importance\"] = clf.feature_importance()\n#     fold_importance_df[\"fold\"] = fold_ + 1\n#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e93e480b7f8404f0a460d4663696ab0b142b1dc","_kg_hide-input":true},"cell_type":"code","source":"print(\"\\n >> CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c65e08b0176a7b4f91ef95ea8e238f48934ba9cf"},"cell_type":"code","source":"# cols = (feature_importance_df[[\"feature\", \"importance\"]]\n#         .groupby(\"feature\")\n#         .mean()\n#         .sort_values(by=\"importance\", ascending=False)[:1000].index)\n# best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\n# plt.figure(figsize=(14,26))\n# sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n# plt.title('LightGBM Features (averaged over folds)')\n# plt.tight_layout()\n# plt.savefig('lgbm_importances.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"757de0c47f13334bde694a06b3b4eba4c3ae9ad9"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_uuid":"b2c5fc1d50c619bc36c1041e63c7a8802ffd8ac0"},"cell_type":"code","source":"submission = pd.DataFrame({\"ID_code\": test.ID_code.values})\nsubmission[\"target\"] = predictions\nsubmission.to_csv(\"santander_LGBM_baseline_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fa54f6ffd8cbff04259aa8f56147655a49414bc"},"cell_type":"code","source":"# submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}